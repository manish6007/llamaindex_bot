import os
import time
import boto3
import pandas as pd
import nest_asyncio
from typing import List
from pydantic import BaseModel
from dotenv import load_dotenv
from datasets import Dataset
from ragas.llms import LangchainLLMWrapper
from ragas.llms.base import BaseRagasLLM
from langchain_core.language_models import BaseLanguageModel
from ragas.metrics import (
    context_precision,
    faithfulness,
    context_recall,
)
from langchain_aws import ChatBedrock
from langchain_community.embeddings import BedrockEmbeddings
from llama_index.core.memory import ChatMemoryBuffer, VectorMemory, SimpleComposableMemory
from llama_index.core import (
    SimpleDirectoryReader, VectorStoreIndex, Settings
)
from llama_index.llms.bedrock import Bedrock
from llama_index.llms.bedrock_converse import BedrockConverse
from llama_index.embeddings.bedrock import BedrockEmbedding
from llama_index.core.agent import ReActAgent
from llama_index.core.tools import FunctionTool

from langchain.embeddings.base import Embeddings
from ragas.evaluation import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision

# --------------------------- Setup ---------------------------
load_dotenv()
nest_asyncio.apply()

# ----------------------- Embedding Wrapper -------------------
class BedrockLangchainEmbedding(Embeddings):
    def __init__(self, bedrock_embed_model):
        self.embed_model = bedrock_embed_model

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self.embed_model._get_text_embeddings(texts)

    def embed_query(self, text: str) -> List[float]:
        return self.embed_model._get_text_embedding(text)

# ---------------------- Load Docs + Index --------------------
documents = SimpleDirectoryReader("data").load_data()
embed_model = BedrockEmbedding(
    model_name='amazon.titan-embed-text-v2:0',
    region_name='us-east-1'
)

index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)
query_engine = index.as_query_engine()

# ------------------------ LLM Setup --------------------------
llm = Bedrock(model="anthropic.claude-3-5-sonnet-20241022-v2:0", temperature=0, region_name="us-east-1",context_size=4096, max_tokens=4096)
Settings.llm = llm

# ------------------------- SQL Tool --------------------------
class SQLResponse(BaseModel):
    sql_query: str
    data: List[dict]
    explanation: str

def execute_sql(query: str) -> SQLResponse:
    client = boto3.client('athena', region_name='us-east-1')
    response = client.start_query_execution(
        QueryString=query,
        QueryExecutionContext={'Database': "athena_db"},
        ResultConfiguration={'OutputLocation': "s3://bedrock-350474408512-us-east-1"}
    )
    query_execution_id = response['QueryExecutionId']

    while True:
        response = client.get_query_execution(QueryExecutionId=query_execution_id)
        state = response['QueryExecution']['Status']['State']
        if state in ['SUCCEEDED', 'FAILED', 'CANCELLED']:
            break
        time.sleep(1)

    if state == 'SUCCEEDED':
        result = client.get_query_results(QueryExecutionId=query_execution_id)
        columns = [col['Name'] for col in result['ResultSet']['ResultSetMetadata']['ColumnInfo']]
        rows = result['ResultSet']['Rows'][1:]
        data = [dict(zip(columns, [item.get('VarCharValue', '') for item in row['Data']])) for row in rows]
        explanation = f"Executed query: {query}. Retrieved {len(data)} records."
        return SQLResponse(sql_query=query, data=data, explanation=explanation)
    else:
        reason = response['QueryExecution']['Status'].get('StateChangeReason', 'Unknown')
        raise Exception(f"Query failed: {reason}")

def process_query(query: str) -> str:
    """
    Processes a query using Bedrock LLM and embeddings.

    Args:
        query (str): The user's query.

    Returns:
        str: The response generated by the LLM.
    """
    # Initialize Bedrock LLM
    llm = BedrockConverse(
            model="us.anthropic.claude-3-sonnet-20240229-v1:0",
            region_name="us-east-1",
        )
    Settings.llm = llm

    # Initialize Bedrock Embedding
    embed_model = BedrockEmbedding(
        aws_access_key_id="",
        aws_secret_access_key="", # Optional, if using temporary credentials
        region_name="us-east-1",
        model_name="amazon.titan-embed-text-v1"
    )

    # Load documents
    documents = SimpleDirectoryReader(input_dir="./data").load_data()

    # Create an index
    index = VectorStoreIndex.from_documents(
        documents=documents,
        embed_model=embed_model,
        llm=llm
    )

    # Create a query engine
    query_engine = index.as_query_engine()

    # Query the index
    response = query_engine.query(query)

    return str(response)

execute_sql_tool = FunctionTool.from_defaults(fn=execute_sql)
query_gen_tool = FunctionTool.from_defaults(fn=process_query, name="QueryProcessor", description="Processes user queries using Bedrock LLM.")


# ----------------------- Agent Setup -------------------------
chat_memory_buffer = ChatMemoryBuffer.from_defaults()
vector_memory = VectorMemory.from_defaults(
    embed_model=embed_model,
    retriever_kwargs={"similarity_top_k": 5}
)
composable_memory = SimpleComposableMemory(
    primary_memory=chat_memory_buffer,
    secondary_memory_sources=[vector_memory],
)

llm_converse = BedrockConverse(
    model="us.anthropic.claude-3-5-sonnet-20241022-v2:0",
    region_name="us-east-1"
)

agent_context = """
You are an expert in writing Athena ANSI SQL standard queries. You can answer database-related queries compatible with AWS Athena. Use the QueryProcessor to prepare SQL queries. If the question is not directly related to the database or SQL, retrieve the relevant information from your conversation memory. You can also use the SQL execution tool to execute SQL queries and retrieve results. Run limit queries to get sample data from the database. Use CAST in case of any type mismatch in the query. 
Always return Final ans with below format:
Final Answer: { "sql_query": "...", "data": [...], "explanation": "..." }
"""

agent = ReActAgent(
    tools=[execute_sql_tool,query_gen_tool],
    llm=llm_converse,
    memory=composable_memory,
    max_iterations=20,
    context=agent_context,
    verbose=True,
)

bedrock_model = ChatBedrock(
    region_name="us-east-1",
    endpoint_url=f"https://bedrock-runtime.us-east-1.amazonaws.com",
    model_id="anthropic.claude-3-sonnet-20240229-v1:0",
    model_kwargs={"temperature": 0.4},
)

# init the embeddings
bedrock_embeddings = BedrockLangchainEmbedding(
    bedrock_embed_model=embed_model
)

from ragas import evaluate
import nest_asyncio  # CHECK NOTES

# NOTES: Only used when running on a jupyter notebook, otherwise comment or remove this function.
nest_asyncio.apply()

# eval_results = evaluate(
#     dag_dataset,
#     metrics=[faithfulness, answer_relevancy, context_precision],
#     embeddings=bedrock_embeddings,
#     llm=bedrock_model
# )

# ------------------- Chat + Evaluate + Save --------------------
queries = [
    'tell me about all the customers',
    'tell me about all the unique customers who placed orders?',
    'tell me about the product which generated maximum revenue?'
]

results_list = []

for query in queries:
    print(f"\nQuery: {query}")
    
    response = agent.chat(query)
    print("\nAgent Response:\n", response)

    # Process with RAG context retriever
    knowledebase_resp = process_query(query)

    # Prepare sample for RAGAS evaluation
    sample = {
        "question": query,
        "answer": response.response if hasattr(response, "response") else str(response),
        "contexts": [node.text for node in knowledebase_resp.source_nodes] if hasattr(knowledebase_resp, "source_nodes") else [doc.text for doc in documents[:3]],
        "reference": response.response if hasattr(response, "response") else str(response),
    }

    print("\nSample for Evaluation:\n", sample)

    dag_dataset = Dataset.from_list([sample])

    eval_results = evaluate(
        dag_dataset,
        metrics=[faithfulness, answer_relevancy, context_precision],
        embeddings=bedrock_embeddings,
        llm=bedrock_model
    )

    df_result = eval_results.to_pandas()
    df_result["query"] = query
    results_list.append(df_result)

# Concatenate all results and write to CSV
final_df = pd.concat(results_list, ignore_index=True)
final_df.to_csv("evaluation_results.csv", index=False)
print("\nAll RAGAS Evaluations saved to evaluation_results.csv")